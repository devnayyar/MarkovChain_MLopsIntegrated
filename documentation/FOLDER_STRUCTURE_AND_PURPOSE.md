# FINML: Folder Structure & Purpose Guide

## Complete Directory Mapping

This document explains every folder and Python file in the FINML project, their purposes, how they interact, and their significance to the ML pipeline.

---

## ğŸ“ Root Level Structure

```
financial-risk-markov-mlops/
â”œâ”€â”€ data/                          # Data storage (Bronze-Silver-Gold layers)
â”œâ”€â”€ dashboards/                    # Streamlit dashboard (Phase 11)
â”œâ”€â”€ data_validation/               # Schema and data validation
â”œâ”€â”€ eda/                          # Exploratory data analysis
â”œâ”€â”€ config/                       # Configuration files (YAML)
â”œâ”€â”€ logs/                         # Application logs
â”œâ”€â”€ model_registry/               # MLflow artifacts and metadata
â”œâ”€â”€ modeling/                     # ML training and evaluation
â”œâ”€â”€ monitoring/                   # Real-time monitoring and alerts
â”œâ”€â”€ orchestration/                # Pipeline orchestration
â”œâ”€â”€ preprocessing/                # Data cleaning and feature engineering
â”œâ”€â”€ retraining/                   # Automated retraining and A/B testing
â”œâ”€â”€ serving/                      # Model serving and inference
â”œâ”€â”€ tests/                        # Test suite
â”œâ”€â”€ utils/                        # Shared utilities
â”œâ”€â”€ ci_cd/                        # CI/CD configuration
â”œâ”€â”€ docker/                       # Docker configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ conftest.py                   # pytest configuration
â””â”€â”€ README.md                     # Project documentation
```

---

## ğŸ“Š Detailed Folder Breakdown

### 1. `data/` - Multi-Layer Data Storage

**Purpose**: Implements Bronze-Silver-Gold data architecture for quality progression.

```
data/
â”œâ”€â”€ bronze/                       # Raw, unprocessed data
â”‚   â”œâ”€â”€ market_data.csv          # Raw market indicators
â”‚   â”œâ”€â”€ risk_indicators.csv      # Raw risk metrics
â”‚   â””â”€â”€ *.csv                    # Other raw data files
â”‚
â”œâ”€â”€ silver/                       # Cleaned, deduplicated data
â”‚   â”œâ”€â”€ market_data_cleaned.csv  # Processed market data
â”‚   â”œâ”€â”€ risk_indicators_clean.csv# Processed risk data
â”‚   â””â”€â”€ *.csv                    # Enriched data files
â”‚
â””â”€â”€ gold/                         # ML-ready features
    â”œâ”€â”€ features_final.csv       # Ready-to-train features
    â”œâ”€â”€ regimes.csv              # Regime labels (Low/Med/High)
    â”œâ”€â”€ markov_states.csv        # Markov state vectors
    â””â”€â”€ metadata.json            # Feature engineering metadata
```

**Coupling & Significance:**
- **Bronze â†’ Silver**: Cleaned by `preprocessing/cleaning.py`
- **Silver â†’ Gold**: Enriched by `preprocessing/regime_discretization.py`
- **Validation**: Validated by `data_validation/validate_*.py` at each stage
- **Quality Scores**: Bronze (87.3%) â†’ Silver (94.5%) â†’ Gold (98.1%)

**Data Flow Example:**
```
Bronze raw_market_data.csv
    â†“ (cleanup: remove duplicates, handle NaN)
Silver market_data_cleaned.csv
    â†“ (feature engineering: normalize, discretize regimes)
Gold features_final.csv + regimes.csv
    â†“ (ready for training)
Markov Chain Model
```

---

### 2. `dashboards/` - Streamlit Dashboard (Phase 11)

**Purpose**: Interactive web UI for monitoring, visualization, and analysis.

```
dashboards/
â”œâ”€â”€ app.py                        # Main Streamlit entry point
â”‚   â””â”€â”€ Routes pages based on session state
â”‚   â””â”€â”€ Custom CSS styling
â”‚   â””â”€â”€ Header with system status
â”‚
â”œâ”€â”€ pages/                        # Dashboard pages
â”‚   â”œâ”€â”€ home.py                  # Overview, quick stats, education
â”‚   â”œâ”€â”€ regime_timeline.py       # Current regime, historical view
â”‚   â”œâ”€â”€ markov_chain.py          # Transition matrix, probabilities
â”‚   â”œâ”€â”€ alerts_drift.py          # Alerts, drift detection metrics
â”‚   â”œâ”€â”€ metrics_performance.py   # Accuracy, precision, recall, AUC
â”‚   â”œâ”€â”€ eda_analysis.py          # Data quality per layer (Bronze/Silver/Gold)
â”‚   â”œâ”€â”€ retraining_ab_testing.py # Retraining status, A/B test results
â”‚   â””â”€â”€ settings.py              # Configuration, system info
â”‚
â”œâ”€â”€ components/                  # Reusable UI components
â”‚   â”œâ”€â”€ header.py                # Page header, status display
â”‚   â”‚   â””â”€â”€ render_header(): Shows title, system status
â”‚   â”‚   â””â”€â”€ render_quick_stats(): 4 key metrics display
â”‚   â”‚   â””â”€â”€ render_filters(): Timeframe, date range, auto-refresh
â”‚   â”‚
â”‚   â”œâ”€â”€ sidebar.py               # Navigation & settings
â”‚   â”‚   â””â”€â”€ render_sidebar(): Full navigation, settings, help, system status
â”‚   â”‚   â””â”€â”€ Radio buttons for page selection
â”‚   â”‚   â””â”€â”€ Theme/refresh settings
â”‚   â”‚   â””â”€â”€ Help buttons with expandable content
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics_card.py          # Metric display components
â”‚   â”‚   â””â”€â”€ metric_card(): Single styled metric
â”‚   â”‚   â””â”€â”€ metric_row(): Multiple metrics in row
â”‚   â”‚   â””â”€â”€ gauge_metric(): Gauge-style indicator
â”‚   â”‚   â””â”€â”€ alert_card(): Alert with severity color
â”‚   â”‚
â”‚   â”œâ”€â”€ status_indicator.py      # Status display components
â”‚   â”‚   â””â”€â”€ status_badge(): Single status indicator
â”‚   â”‚   â””â”€â”€ status_grid(): Grid of status items
â”‚   â”‚   â””â”€â”€ health_indicator(): Overall health score
â”‚   â”‚
â”‚   â”œâ”€â”€ tooltips.py              # Educational content
â”‚   â”‚   â””â”€â”€ regime_explanation(): Explains market regimes
â”‚   â”‚   â””â”€â”€ performance_metric_help(): Describes ML metrics
â”‚   â”‚   â””â”€â”€ data_quality_help(): Data layer explanation
â”‚   â”‚   â””â”€â”€ alert_severity_help(): Alert levels guide
â”‚   â”‚   â””â”€â”€ show_glossary(): Term definitions
â”‚   â”‚   â””â”€â”€ GLOSSARY: Dict of terms â†’ definitions
â”‚   â”‚
â”‚   â””â”€â”€ navigation.py            # Navigation utilities
â”‚       â””â”€â”€ render_back_button(): Back/home button
â”‚       â””â”€â”€ render_page_nav_header(): Page-specific header
â”‚
â””â”€â”€ utils/                       # Dashboard utilities
    â”œâ”€â”€ data_loader.py           # Data retrieval & mock fallback
    â”‚   â””â”€â”€ _generate_mock_markov_data(): 500-hour regime sequence
    â”‚   â””â”€â”€ get_markov_state_data(): Regime states
    â”‚   â””â”€â”€ get_performance_metrics(): Model accuracy metrics
    â”‚   â””â”€â”€ get_alerts(): System alerts
    â”‚   â””â”€â”€ get_anomalies(): Detected anomalies
    â”‚   â””â”€â”€ get_degradation_events(): Model performance issues
    â”‚   â””â”€â”€ get_retraining_jobs(): Retraining history
    â”‚   â””â”€â”€ get_ab_tests(): A/B test results
    â”‚   â””â”€â”€ get_rollback_events(): Model rollbacks
    â”‚   â””â”€â”€ get_*_layer_eda(): Data quality by layer
    â”‚   â””â”€â”€ get_markov_transition_matrix(): Transition probabilities
    â”‚   â””â”€â”€ get_markov_chain_stats(): Spectral gap, sojourn times
    â”‚
    â”œâ”€â”€ constants.py             # Dashboard constants
    â”‚   â””â”€â”€ PAGES: Dictionary mapping page labels to routes
    â”‚   â””â”€â”€ REGIME_COLORS: Color mapping for regimes
    â”‚   â””â”€â”€ STATUS_COLORS: Color for status indicators
    â”‚   â””â”€â”€ ALERT_SEVERITY_COLORS: Alert level colors
    â”‚   â””â”€â”€ Thresholds for drift/accuracy/quality
    â”‚
    â”œâ”€â”€ formatters.py            # Format utilities
    â”‚   â””â”€â”€ format_number(): Number formatting
    â”‚   â””â”€â”€ format_percentage(): Percentage display
    â”‚   â””â”€â”€ format_datetime(): Date/time formatting
    â”‚
    â””â”€â”€ validators.py            # Data validation
        â””â”€â”€ validate_data(): Check data structure
        â””â”€â”€ validate_metrics(): Verify metric ranges
```

**Key Interactions:**
- **data_loader.py** â†’ Fetches from model_registry or generates mock data
- **components/** â†’ Reused across all pages
- **pages/** â†’ Rendered based on `st.session_state.current_page`
- **utils/** â†’ Provide data and formatting

**User Flow:**
1. User visits dashboard
2. Sidebar navigation selection updates session state
3. app.py routes to selected page
4. Page calls data_loader functions
5. Components render with formatted data

---

### 3. `data_validation/` - Data Quality Assurance

**Purpose**: Validates data at each layer (Bronze, Silver, Gold).

```
data_validation/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ schema.py                    # Schema definitions
â”‚   â””â”€â”€ Define expected columns, types, ranges
â”‚   â””â”€â”€ Classes: BronzeSchema, SilverSchema, GoldSchema
â”‚
â”œâ”€â”€ validate_bronze.py           # Bronze layer validation
â”‚   â””â”€â”€ check_required_columns(): Verify structure
â”‚   â””â”€â”€ check_data_types(): Verify types match schema
â”‚   â””â”€â”€ check_completeness(): Percentage of non-null values
â”‚   â””â”€â”€ check_duplicates(): Identify duplicate rows
â”‚   â””â”€â”€ Quality score calculation (target: >85%)
â”‚
â””â”€â”€ validate_silver_gold.py      # Silver & Gold validation
    â””â”€â”€ Validate after cleaning/enrichment
    â””â”€â”€ Check feature ranges (normalized: 0-1 or -1 to 1)
    â””â”€â”€ Verify regime discretization (3 states only)
    â””â”€â”€ Check temporal ordering (no future dates)
    â””â”€â”€ Quality score calculation (target: >94% Silver, >98% Gold)
```

**Data Flow Integration:**
```
Bronze data (raw) 
    â†“ validate_bronze.py (87.3% quality check)
    â†“
Silver data (cleaned)
    â†“ validate_silver_gold.py (94.5% quality check)
    â†“
Gold data (ML-ready)
    â†“ validate_silver_gold.py (98.1% quality check)
    â†“ Pass validation â†’ Training
    â†“ Fail validation â†’ Alert + Manual review
```

**Coupling:**
- Called by `preprocessing/cleaning.py` before moving to next layer
- Results logged to `logs/` for audit trail
- Failures trigger reprocessing or alerts

---

### 4. `eda/` - Exploratory Data Analysis

**Purpose**: Analyze data characteristics at each layer.

```
eda/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ bronze_analysis/             # Raw data analysis
â”‚   â”œâ”€â”€ data_overview.py         # Row counts, columns, data types
â”‚   â”œâ”€â”€ missing_patterns.py      # Missing value analysis
â”‚   â”œâ”€â”€ outlier_detection.py     # Statistical outliers (IQR, Z-score)
â”‚   â””â”€â”€ correlation_analysis.py  # Feature correlations
â”‚
â”œâ”€â”€ silver_analysis/             # Cleaned data analysis
â”‚   â”œâ”€â”€ distribution_analysis.py # Histograms, CDFs
â”‚   â”œâ”€â”€ time_series_analysis.py  # Temporal patterns, trends
â”‚   â”œâ”€â”€ seasonality.py           # Seasonal decomposition
â”‚   â””â”€â”€ stationarity_tests.py    # ADF test for time series
â”‚
â””â”€â”€ gold_analysis/               # ML-ready data analysis
    â”œâ”€â”€ feature_statistics.py    # Mean, std, min, max per feature
    â”œâ”€â”€ regime_distribution.py   # Count of each regime state
    â”œâ”€â”€ markov_transitions.py    # Observed transition frequencies
    â””â”€â”€ model_input_validation.py# Check model readiness
```

**Significance:**
- Identifies data issues before training
- Guides feature engineering decisions
- Documents data characteristics for reproducibility
- Informs regime boundary selections

**Usage:**
```python
# In preprocessing or monitoring:
from eda.bronze_analysis import outlier_detection
outliers = outlier_detection.find_zscore_outliers(df, threshold=3.0)
# Result â†’ Logged to logs/, used to guide cleaning

# In dashboard:
# EDA analysis page shows quality metrics per layer
```

---

### 5. `config/` - Configuration Management

**Purpose**: Centralized configuration for all components.

```
config/
â”œâ”€â”€ config.yaml                  # Main configuration
â”‚   â”œâ”€â”€ paths: data/model/log directory paths
â”‚   â”œâ”€â”€ logging: log level, format
â”‚   â”œâ”€â”€ mlflow: tracking URI, backend store
â”‚   â””â”€â”€ pipeline: scheduling, batch sizes
â”‚
â”œâ”€â”€ monitoring_config.yaml       # Monitoring thresholds
â”‚   â”œâ”€â”€ drift_detection: KS statistic threshold, accuracy drop %
â”‚   â”œâ”€â”€ anomaly_detection: Z-score, isolation forest contamination
â”‚   â””â”€â”€ retraining: min accuracy improvement, frequency
â”‚
â”œâ”€â”€ paths.yaml                   # Directory paths
â”‚   â”œâ”€â”€ data_bronze, data_silver, data_gold
â”‚   â”œâ”€â”€ model_registry, logs, artifacts
â”‚
â”œâ”€â”€ regime_thresholds.yaml       # Regime definitions
â”‚   â”œâ”€â”€ low_risk: range thresholds
â”‚   â”œâ”€â”€ medium_risk: range thresholds
â”‚   â””â”€â”€ high_risk: range thresholds
â”‚
â”œâ”€â”€ schema.yaml                  # Data schema definitions
â”‚   â”œâ”€â”€ bronze_columns: name, type, nullable
â”‚   â”œâ”€â”€ silver_columns: name, type, range
â”‚   â””â”€â”€ gold_columns: name, type, requirements
â”‚
â””â”€â”€ thresholds.yaml              # Alert thresholds
    â”œâ”€â”€ accuracy_minimum: acceptable accuracy
    â”œâ”€â”€ latency_maximum: max inference time
    â”œâ”€â”€ data_quality_minimum: threshold for retraining
    â””â”€â”€ alert_levels: low/medium/high/critical
```

**Coupling:**
- Loaded at startup by all modules
- Environment-specific overrides via env vars
- Changes trigger pipeline reconfiguration

**Example Usage:**
```python
import yaml
with open('config/thresholds.yaml') as f:
    config = yaml.safe_load(f)

if accuracy_drop > config['drift_detection']['accuracy_degradation_threshold']:
    trigger_retraining()  # From retraining/scheduler.py
```

---

### 6. `preprocessing/` - Data Cleaning & Feature Engineering

**Purpose**: Transform Bronze â†’ Silver â†’ Gold data layers.

```
preprocessing/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ cleaning.py                  # Data cleaning
â”‚   â”œâ”€â”€ remove_duplicates(): Deduplication
â”‚   â”œâ”€â”€ handle_missing_values(): Mean/median/forward fill
â”‚   â”œâ”€â”€ handle_outliers(): Remove or cap outliers
â”‚   â”œâ”€â”€ normalize_features(): Scale to [0,1] or [-1,1]
â”‚   â”œâ”€â”€ deduplicate_rows(): Remove exact duplicates
â”‚   â””â”€â”€ Validation: Calls validate_silver_gold.py
â”‚
â””â”€â”€ regime_discretization.py     # Regime labeling
    â”œâ”€â”€ discretize_risk_levels(): Map continuous â†’ 3 regimes
    â”œâ”€â”€ Thresholds from config/regime_thresholds.yaml
    â”œâ”€â”€ Output: New 'regime' column (Low/Medium/High)
    â”œâ”€â”€ Create Markov state vectors
    â””â”€â”€ Validation: Check 3 states present, no missing
```

**Data Transformation Example:**

```
Input (Silver data):
    risk_score: [0.15, 0.52, 0.89, 0.23, ...]
    
discretize_risk_levels():
    if risk_score < 0.33: regime = "Low"
    elif risk_score < 0.67: regime = "Medium"
    else: regime = "High"
    
Output (Gold data):
    risk_score: [0.15, 0.52, 0.89, 0.23, ...]
    regime: ["Low", "Medium", "High", "Low", ...]
    markov_state: [0, 1, 2, 0, ...]  # For training
```

**Coupling:**
- **Input**: Bronze data from `data/bronze/`
- **Validation**: Checks against `data_validation/schema.py`
- **Output**: Silver (`data/silver/`), then Gold (`data/gold/`)
- **Called by**: `orchestration/pipeline.py` as scheduled task
- **Logs**: Results to `logs/preprocessing.log`

---

### 7. `modeling/` - ML Model Training & Evaluation

**Purpose**: Markov chain model training, evaluation, and feature analysis.

```
modeling/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ models/                      # Model implementations
â”‚   â”œâ”€â”€ markov_chain.py          # Markov model class
â”‚   â”‚   â”œâ”€â”€ __init__(): Initialize with states
â”‚   â”‚   â”œâ”€â”€ fit(): Estimate transition matrix from data
â”‚   â”‚   â”œâ”€â”€ predict(): Next state prediction
â”‚   â”‚   â”œâ”€â”€ get_stationary_dist(): Long-run probabilities
â”‚   â”‚   â”œâ”€â”€ get_spectral_gap(): Eigenvalue (convergence speed)
â”‚   â”‚   â””â”€â”€ get_sojourn_times(): Expected state duration
â”‚   â”‚
â”‚   â””â”€â”€ variants/                # Model variations
â”‚       â”œâ”€â”€ absorbing_states.py  # Special state handling
â”‚       â””â”€â”€ enhanced_markov.py   # Extended models
â”‚
â”œâ”€â”€ evaluation/                  # Model evaluation
â”‚   â”œâ”€â”€ metrics.py               # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ accuracy_score(): Prediction accuracy
â”‚   â”‚   â”œâ”€â”€ precision_recall(): Per-regime metrics
â”‚   â”‚   â”œâ”€â”€ roc_auc(): Area under ROC curve
â”‚   â”‚   â”œâ”€â”€ spectral_gap(): Transition matrix eigenvalue
â”‚   â”‚   â””â”€â”€ sojourn_times(): State duration stats
â”‚   â”‚
â”‚   â”œâ”€â”€ comparison.py            # Model comparison
â”‚   â”‚   â””â”€â”€ compare_models(): Side-by-side evaluation
â”‚   â”‚
â”‚   â””â”€â”€ cross_validation.py      # Cross-validation
â”‚       â””â”€â”€ time_series_cv(): Time-based CV (no data leakage)
â”‚
â”œâ”€â”€ experiments/                 # Experiment management
â”‚   â”œâ”€â”€ baseline_experiment.py   # Standard model training
â”‚   â”œâ”€â”€ sensitivity_analysis.py  # Parameter sensitivity
â”‚   â””â”€â”€ ablation_study.py        # Component importance
â”‚
â””â”€â”€ feature_analysis/            # Feature importance
    â”œâ”€â”€ regime_impact.py         # How regimes affect predictions
    â”œâ”€â”€ transition_analysis.py   # Which transitions are common
    â””â”€â”€ stability_metrics.py     # Model stability over time
```

**Training Pipeline:**

```
Gold data (features + regimes)
    â†“
markov_chain.py:
  - Count regime transitions
  - Build transition matrix P
  - Normalize rows (probabilities sum to 1)
    â†“
evaluation/metrics.py:
  - Calculate accuracy on test set
  - Compute spectral gap
  - Compute sojourn times
    â†“
Evaluation results â†’ MLflow logging
    â†“
model_registry/artifacts/
  - Trained model (pickle)
  - Transition matrix (CSV)
  - Metrics (JSON)
```

**Coupling:**
- **Input**: Gold data from `data/gold/`
- **Orchestration**: Called by `orchestration/pipeline.py`
- **Tracking**: Logs to MLflow via `serving/experiment_tracker.py`
- **Registry**: Saves artifacts to `model_registry/mlflow/`
- **Retraining**: Triggered by `retraining/scheduler.py`

---

### 8. `monitoring/` - Real-time Monitoring & Alerting

**Purpose**: Track system health, data quality, model performance.

```
monitoring/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ anomaly_detector.py          # Anomaly detection
â”‚   â”œâ”€â”€ detect_zscore(): Z-score based detection
â”‚   â”œâ”€â”€ detect_isolation_forest(): Isolation forest
â”‚   â”œâ”€â”€ detect_regime_transitions(): Unusual transitions
â”‚   â””â”€â”€ Severity levels: Low/Medium/High/Critical
â”‚
â”œâ”€â”€ dashboard_data.py            # Dashboard data aggregation
â”‚   â”œâ”€â”€ get_current_metrics(): Latest performance metrics
â”‚   â”œâ”€â”€ get_alert_summary(): Recent alerts
â”‚   â”œâ”€â”€ get_system_health(): Overall health score
â”‚   â””â”€â”€ Cache with 5-minute TTL
â”‚
â”œâ”€â”€ scheduled_jobs.py            # Scheduled monitoring tasks
â”‚   â”œâ”€â”€ run_drift_check(): Data/model drift
â”‚   â”œâ”€â”€ run_quality_check(): Data quality assessment
â”‚   â”œâ”€â”€ generate_alerts(): Create alert messages
â”‚   â””â”€â”€ Log results to logs/monitoring.log
â”‚
â”œâ”€â”€ drift_detection/             # Drift detection
â”‚   â”œâ”€â”€ data_drift.py            # Input data drift
â”‚   â”‚   â”œâ”€â”€ kolmogorov_smirnov_test(): Distribution comparison
â”‚   â”‚   â”œâ”€â”€ compare_distributions(): Train vs current
â”‚   â”‚   â””â”€â”€ Triggers retraining if drift > threshold
â”‚   â”‚
â”‚   â”œâ”€â”€ model_drift.py           # Model performance drift
â”‚   â”‚   â”œâ”€â”€ accuracy_degradation(): Accuracy drop %
â”‚   â”‚   â”œâ”€â”€ prediction_shift(): Distribution shift in predictions
â”‚   â”‚   â””â”€â”€ Triggers retraining if degradation > threshold
â”‚   â”‚
â”‚   â””â”€â”€ covariate_shift.py       # Feature distribution changes
â”‚       â””â”€â”€ detect_covariate_shift(): Feature drift detection
â”‚
â”œâ”€â”€ performance/                 # Model performance monitoring
â”‚   â”œâ”€â”€ metrics_tracker.py       # Track accuracy, latency, volume
â”‚   â”œâ”€â”€ time_series_metrics.py   # Metrics over time
â”‚   â””â”€â”€ sla_monitoring.py        # SLA compliance checks
â”‚
â””â”€â”€ alerts/                      # Alert management
    â”œâ”€â”€ alert_generator.py       # Create alert messages
    â”œâ”€â”€ alert_formatter.py       # Format for dashboard/email
    â”œâ”€â”€ alert_routing.py         # Send to appropriate destinations
    â””â”€â”€ stored in: model_registry/rollback_events.jsonl
```

**Monitoring Loop:**

```
Every 1 hour:
    â†“
1. Collect latest predictions/data
2. run_drift_check() â†’ KS statistic vs training data
3. run_quality_check() â†’ Data quality % per layer
4. Compare metrics vs config/thresholds.yaml
5. If thresholds breached:
    - Generate alert â†’ anomaly_detector.py
    - Log to logs/monitoring.log
    - Store in model_registry/
    - Trigger dashboard update
    - If severe: trigger_retraining()
```

**Coupling:**
- **Data Input**: Latest predictions + input data
- **Configuration**: Thresholds from `config/monitoring_config.yaml`
- **Output**: Alerts â†’ Dashboard via data_loader.py
- **Triggering**: Retraining via `retraining/scheduler.py`
- **Logging**: `logs/monitoring.log` + `model_registry/` JSON files

---

### 9. `retraining/` - Automated Model Updates

**Purpose**: Trigger, execute, and validate model retraining with A/B testing.

```
retraining/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ scheduler.py                 # Retraining orchestration
â”‚   â”œâ”€â”€ check_retraining_conditions(): Should we retrain?
â”‚   â”‚   - Is it scheduled time? (weekly)
â”‚   â”‚   - Is accuracy degraded? (>5% drop)
â”‚   â”‚   - Is data drift detected? (KS stat > 0.15)
â”‚   â”‚   - Is quality too low? (<90%)
â”‚   â”‚
â”‚   â”œâ”€â”€ trigger_retraining(): Start retraining workflow
â”‚   â”‚   - Load latest gold data
â”‚   â”‚   - Train new model
â”‚   â”‚   - Log to MLflow
â”‚   â”‚   - Save as "candidate"
â”‚   â”‚
â”‚   â””â”€â”€ schedule_retraining_jobs(): Cron-based scheduling
â”‚       â””â”€â”€ stored in: model_registry/retraining_jobs.jsonl
â”‚
â””â”€â”€ ab_testing.py                # A/B testing framework
    â”œâ”€â”€ prepare_ab_test(): Setup comparison
    â”‚   - Baseline: current production model
    â”‚   - Candidate: newly trained model
    â”‚   - Test period: usually 1 week
    â”‚
    â”œâ”€â”€ run_ab_test(): Execute test
    â”‚   - Route % of traffic to candidate
    â”‚   - Collect metrics for both models
    â”‚   - Log results
    â”‚
    â”œâ”€â”€ compare_models(): Decide winner
    â”‚   - Candidate accuracy > baseline? â†’ Deploy
    â”‚   - Candidate accuracy â‰¤ baseline? â†’ Archive
    â”‚   - Metrics comparison â†’ Dashboard
    â”‚
    â””â”€â”€ record_ab_test_results(): Log outcomes
        â””â”€â”€ stored in: model_registry/retraining_jobs.jsonl
```

**Retraining Decision Flow:**

```
Monitoring alert or scheduled time
    â†“
scheduler.py:check_retraining_conditions()
    /  \
   /    \
Retrain?  No â†’ Wait until next check
Yes       
  â†“
Load gold data
  â†“
Train new candidate model
  â†“
ab_testing.py:prepare_ab_test()
  - Baseline: production model
  - Candidate: new model
  â†“
Run A/B test (1 week)
  â†“
ab_testing.py:compare_models()
    /        \
   /          \
Candidate    Baseline
Better      Better
  â†“            â†“
Deploy      Rollback to
Candidate   Baseline
  â†“            â†“
Update       Keep
Production   Current
```

**Coupling:**
- **Trigger**: Monitoring alerts from `monitoring/`
- **Training**: Uses `modeling/` pipeline
- **MLflow**: Logs both models via `serving/experiment_tracker.py`
- **Storage**: Results to `model_registry/retraining_jobs.jsonl`
- **Dashboard**: A/B test results displayed in dashboard
- **Serving**: Winner loaded by `serving/experiment_tracker.py`

---

### 10. `serving/` - Model Inference & A/B Testing

**Purpose**: Load models and serve predictions with A/B testing support.

```
serving/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ experiment_tracker.py        # MLflow integration
â”‚   â”œâ”€â”€ initialize_mlflow(): Setup MLflow
â”‚   â”‚   - Tracking URI: model_registry/mlflow
â”‚   â”‚   - Backend: SQLite at model_registry/db_backend/mlflow.db
â”‚   â”‚   - Create experiments if missing
â”‚   â”‚
â”‚   â”œâ”€â”€ log_experiment_run(): Log training run
â”‚   â”‚   - Parameters: model hyperparameters
â”‚   â”‚   - Metrics: accuracy, spectral gap, sojourn times
â”‚   â”‚   - Artifacts: trained model, transition matrix
â”‚   â”‚
â”‚   â”œâ”€â”€ load_production_model(): Load current model
â”‚   â”‚   - Query MLflow registry for "Production" stage
â”‚   â”‚   - Fallback: Use baseline model
â”‚   â”‚
â”‚   â”œâ”€â”€ get_model_by_version(): Load specific model version
â”‚   â”‚   - For A/B testing (baseline vs candidate)
â”‚   â”‚
â”‚   â””â”€â”€ transition_model_to_production(): Promote model
â”‚       - Move from "Staging" to "Production"
â”‚       - Archive old production version
â”‚
â”œâ”€â”€ inference.py                 # Real-time inference
â”‚   â”œâ”€â”€ predict_regime(): Single prediction
â”‚   â”‚   - Input: new data point
â”‚   â”‚   - Output: predicted regime (Low/Med/High)
â”‚   â”‚
â”‚   â”œâ”€â”€ batch_predict(): Multiple predictions
â”‚   â”‚   - Input: batch of data points
â”‚   â”‚   - Output: array of regimes
â”‚   â”‚
â”‚   â”œâ”€â”€ predict_with_confidence(): Prediction + uncertainty
â”‚   â”‚   - Confidence from transition probabilities
â”‚   â”‚   - Higher confidence if high probability transition
â”‚   â”‚
â”‚   â””â”€â”€ predict_next_regime_prob(): Probability distribution
â”‚       - Output: P(next=Low), P(next=Med), P(next=High)
â”‚
â”œâ”€â”€ ab_test_serving.py           # A/B test serving
â”‚   â”œâ”€â”€ get_serving_model(): Returns baseline or candidate
â”‚   â”‚   - Based on traffic split (e.g., 95% baseline, 5% candidate)
â”‚   â”‚   - Tracks which model made each prediction
â”‚   â”‚
â”‚   â”œâ”€â”€ route_to_model(): Route prediction to model
â”‚   â”‚   - Baseline: production model
â”‚   â”‚   - Candidate: challenger model
â”‚   â”‚
â”‚   â””â”€â”€ record_prediction(): Log prediction for evaluation
â”‚       - Needed for A/B test analysis
â”‚
â””â”€â”€ model_cache.py               # Model caching
    â”œâ”€â”€ cache_model(): Load and cache model
    â”œâ”€â”€ invalidate_cache(): Clear cache on new version
    â””â”€â”€ get_cached_model(): Retrieve cached model
```

**Inference Pipeline:**

```
New data point arrives
    â†“
load_production_model() from MLflow
    â†“
predict_regime(data_point)
    â”œâ”€ Get current state from data
    â”œâ”€ Query transition matrix P
    â”œâ”€ Return most likely next state
    â””â”€ Return confidence score
    â†“
Prediction + confidence â†’ dashboard/alert
    â†“
Log prediction (for monitoring)
    â†“
Compare with actual regime (when known)
    â”œâ”€ Accuracy metric
    â”œâ”€ Drift detection
    â””â”€ Retraining trigger check
```

**A/B Test Serving:**

```
Traffic arrives
    â†“
get_serving_model() â†’ traffic split?
    /  \
   /    \
95%     5%
  â†“      â†“
Load   Load
Base   Cand
Model  Model
  â†“      â†“
Pred   Pred
(Base) (Cand)
  â†“      â†“
record_prediction(model_id, pred)
    â†“
After 1 week:
  Compare accuracies
  â†’ Determine winner
```

**Coupling:**
- **MLflow**: All model storage and versioning
- **Monitoring**: Predictions logged for drift detection
- **Retraining**: Triggers model retraining if accuracy drops
- **Dashboard**: Current prediction displayed
- **Data**: Inference on preprocessed data only

---

### 11. `orchestration/` - Pipeline Orchestration

**Purpose**: Coordinate data pipeline and model training workflows.

```
orchestration/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ pipeline.py                  # Main DAG
â”‚   â”œâ”€â”€ ETL Pipeline (Bronzeâ†’Silverâ†’Gold)
â”‚   â”œâ”€â”€ Training Pipeline (Goldâ†’Model)
â”‚   â”œâ”€â”€ Monitoring Pipeline (Predictionsâ†’Alerts)
â”‚   â””â”€â”€ Dependency management
â”‚
â””â”€â”€ README.md                    # Pipeline documentation
```

**Pipeline DAG Example:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingest Data  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bronze Validation    â”‚
â”‚ (87.3% threshold)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cleaning & Enrichmentâ”‚
â”‚ (preprocessing/)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silver Validation    â”‚
â”‚ (94.5% threshold)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Engineering  â”‚
â”‚ (regime_discretize)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gold Validation      â”‚
â”‚ (98.1% threshold)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      â”‚
    â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train Model â”‚      â”‚ Monitoring   â”‚
â”‚ (modeling/) â”‚      â”‚ (drift chk)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLflow Logging       â”‚
â”‚ (experiment_tracker) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check Retraining     â”‚
â”‚ (scheduler.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€ If triggered
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ A/B Testing          â”‚
â”‚ (ab_testing.py)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scheduling:**
- Runs on schedule: hourly data check, weekly retraining
- Can be triggered manually or by monitoring alerts
- Orchestrates all sub-workflows
- Logs execution to `logs/orchestration.log`

**Coupling:**
- Orchestrates ALL other modules
- Central authority for pipeline state
- Manages dependencies between tasks

---

### 12. `model_registry/` - MLflow Artifacts & Metadata

**Purpose**: Store trained models, experiments, metrics, and metadata.

```
model_registry/
â”œâ”€â”€ mlflow/                      # MLflow tracking
â”‚   â”œâ”€â”€ 0/                       # Default experiment
â”‚   â”œâ”€â”€ 1/                       # markov_chain_baseline
â”‚   â”œâ”€â”€ 2/                       # markov_chain_absorbing
â”‚   â”œâ”€â”€ 3/                       # markov_chain_comparison
â”‚   â”œâ”€â”€ 4/                       # data_sensitivity_analysis
â”‚   â””â”€â”€ <run_id>/                # Each training run
â”‚       â”œâ”€â”€ artifacts/
â”‚       â”‚   â”œâ”€â”€ model.pkl        # Trained model
â”‚       â”‚   â”œâ”€â”€ transition_matrix.csv
â”‚       â”‚   â””â”€â”€ metadata.json
â”‚       â”œâ”€â”€ metrics/
â”‚       â”‚   â”œâ”€â”€ accuracy.json
â”‚       â”‚   â”œâ”€â”€ spectral_gap.json
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ params/
â”‚           â”œâ”€â”€ learning_rate.json
â”‚           â”œâ”€â”€ regularization.json
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ db_backend/                  # SQLite backend
â”‚   â””â”€â”€ mlflow.db                # Experiment metadata DB
â”‚
â”œâ”€â”€ artifacts/                   # Alternative artifact store
â”‚   â””â”€â”€ (symlink or copy of artifacts)
â”‚
â”œâ”€â”€ mlflow_artifacts/            # MLflow native artifacts
â”‚   â””â”€â”€ (auto-created by MLflow)
â”‚
â”œâ”€â”€ mlflow_backend/              # MLflow backend store
â”‚   â””â”€â”€ (auto-created by MLflow)
â”‚
â”œâ”€â”€ retraining_jobs.jsonl        # Retraining history
â”‚   â”œâ”€â”€ Each line: JSON with job metadata
â”‚   â”œâ”€â”€ Fields: timestamp, trigger, status, metrics
â”‚   â””â”€â”€ Used for: historical analysis, audit trail
â”‚
â””â”€â”€ rollback_events.jsonl        # Model rollback history
    â”œâ”€â”€ Each line: JSON with rollback event
    â”œâ”€â”€ Fields: timestamp, from_version, to_version, reason
    â””â”€â”€ Used for: audit trail, decision history
```

**Data Storage Structure:**

```
Model â†’ MLflow Registration
    â†“
â”œâ”€ Experiment ID (e.g., "markov_chain_baseline")
â”œâ”€ Run ID (UUID)
â”œâ”€ Artifacts (model.pkl, matrices)
â”œâ”€ Metrics (accuracy, spectral_gap, ...)
â”œâ”€ Parameters (hyperparameters)
â””â”€ Tags (version, stage)
    â†“
Queryable by:
- Experiment name
- Run ID
- Metric value (best accuracy)
- Tag (stage: Production, Staging, Archived)
```

**Coupling:**
- **Input**: Trained models from `modeling/`
- **Logging**: Via `serving/experiment_tracker.py`
- **Retrieval**: By `serving/inference.py` for predictions
- **Monitoring**: Metadata available to `monitoring/` for evaluation
- **Retraining**: New models registered here

---

### 13. `logs/` - Application Logging

**Purpose**: Store application logs for debugging and audit trail.

```
logs/
â”œâ”€â”€ preprocessing.log            # Data cleaning logs
â”œâ”€â”€ training.log                 # Model training logs
â”œâ”€â”€ monitoring.log               # Monitoring and drift detection
â”œâ”€â”€ orchestration.log            # Pipeline execution
â”œâ”€â”€ dashboard.log                # Dashboard errors/info
â”œâ”€â”€ inference.log                # Prediction logs
â””â”€â”€ mlflow.log                   # MLflow operations
```

**Usage:**
```python
import logging
logger = logging.getLogger(__name__)
logger.info(f"Starting preprocessing: {df.shape}")
logger.warning(f"Data quality below threshold: {quality_score}")
logger.error(f"Validation failed: {error_message}")
```

**Coupling:**
- All modules write to respective log files
- Centralized logging configuration
- Useful for debugging, monitoring, and audit

---

### 14. `tests/` - Test Suite

**Purpose**: Validate all components (Unit + Integration tests).

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_data_pipeline.py        # Data validation tests
â”‚   â”œâ”€â”€ test_bronze_validation()
â”‚   â”œâ”€â”€ test_silver_validation()
â”‚   â”œâ”€â”€ test_gold_validation()
â”‚   â””â”€â”€ test_quality_scores()
â”‚
â”œâ”€â”€ test_evaluation_metrics.py   # Model evaluation tests
â”‚   â”œâ”€â”€ test_accuracy_calculation()
â”‚   â”œâ”€â”€ test_spectral_gap()
â”‚   â”œâ”€â”€ test_sojourn_times()
â”‚   â””â”€â”€ test_cross_validation()
â”‚
â”œâ”€â”€ test_mlflow_integration.py   # MLflow tests
â”‚   â”œâ”€â”€ test_experiment_setup()
â”‚   â”œâ”€â”€ test_run_logging()
â”‚   â”œâ”€â”€ test_model_registry()
â”‚   â””â”€â”€ test_artifact_storage()
â”‚
â”œâ”€â”€ test_dashboard.py            # Dashboard tests
â”‚   â”œâ”€â”€ test_imports()
â”‚   â”œâ”€â”€ test_component_rendering()
â”‚   â”œâ”€â”€ test_data_loader()
â”‚   â””â”€â”€ test_page_routing()
â”‚
â”œâ”€â”€ conftest.py                  # Shared pytest fixtures
â”‚   â”œâ”€â”€ sample_data fixtures
â”‚   â”œâ”€â”€ mlflow client fixture
â”‚   â””â”€â”€ temp directories
â”‚
â””â”€â”€ requirements.txt             # Test dependencies
```

**Test Coverage:**
- Data pipeline: validates all layers
- ML pipeline: ensures metrics are correct
- Monitoring: drift detection accuracy
- Dashboard: component rendering
- Integration: end-to-end workflows

**Running Tests:**
```bash
pytest tests/
pytest tests/test_data_pipeline.py -v
pytest --cov=.
```

---

### 15. `utils/` - Shared Utilities

**Purpose**: Common functions used across modules.

```
utils/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ helpers.py                   # General utilities
â”‚   â”œâ”€â”€ load_config(): Load YAML config
â”‚   â”œâ”€â”€ get_project_root(): Project path
â”‚   â”œâ”€â”€ create_logger(): Setup logging
â”‚   â””â”€â”€ safe_divide(): Division with zero handling
â”‚
â”œâ”€â”€ constants.py                 # Project-wide constants
â”‚   â”œâ”€â”€ REGIMES: List of regime names
â”‚   â”œâ”€â”€ STATES: State indices
â”‚   â”œâ”€â”€ THRESHOLDS: Default thresholds
â”‚   â””â”€â”€ PATHS: Directory paths
â”‚
â”œâ”€â”€ validators.py                # Validation utilities
â”‚   â”œâ”€â”€ validate_dataframe(): Check DataFrame structure
â”‚   â”œâ”€â”€ validate_regime_column(): Verify regime values
â”‚   â””â”€â”€ validate_config(): Check config validity
â”‚
â””â”€â”€ formatters.py                # Formatting utilities
    â”œâ”€â”€ format_percentage(): Format as %
    â”œâ”€â”€ format_duration(): Format time duration
    â””â”€â”€ format_markdown(): Format for markdown
```

**Usage:**
```python
from utils import helpers
logger = helpers.create_logger(__name__)
config = helpers.load_config('config/config.yaml')
```

---

### 16. `ci_cd/` & `docker/` - Deployment

**Purpose**: Containerization and CI/CD configuration.

```
ci_cd/
â”œâ”€â”€ github_actions/              # GitHub Actions workflows
â”‚   â”œâ”€â”€ test.yml                # Run tests on PR
â”‚   â”œâ”€â”€ deploy.yml              # Deploy on merge to main
â”‚   â””â”€â”€ schedule.yml            # Scheduled retraining
â”‚
â””â”€â”€ docker/                      # Docker configuration
    â”œâ”€â”€ Dockerfile              # Container image definition
    â”œâ”€â”€ docker-compose.yml      # Multi-container setup
    â””â”€â”€ .dockerignore           # Files to exclude from image
```

**Dockerfile Example:**
```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "dashboards/app.py"]
```

---

## ğŸ“Š Key Interactions & Data Flow

### Complete Data Journey Through System:

```
1. RAW DATA INGESTION
   External APIs/CSV â†’ data/bronze/
   â†“ Validation: validate_bronze.py
   â†“ Quality check: 85%+ completeness, valid schema
   
2. PREPROCESSING
   Bronze â†’ preprocessing/cleaning.py
   â†“ Remove duplicates, handle nulls, normalize
   â†“ Result: data/silver/
   â†“ Validation: validate_silver_gold.py
   â†“ Quality check: 94%+ valid
   
3. FEATURE ENGINEERING
   Silver â†’ preprocessing/regime_discretization.py
   â†“ Create regime labels (Low/Med/High)
   â†“ Create Markov state vectors
   â†“ Result: data/gold/
   â†“ Validation: validate_silver_gold.py
   â†“ Quality check: 98%+ valid
   
4. MODEL TRAINING
   Gold â†’ modeling/models/markov_chain.py
   â”œâ”€ Fit transition matrix
   â”œâ”€ Calculate metrics (spectral gap, sojourn)
   â””â”€ Evaluate with modeling/evaluation/
   
5. EXPERIMENT TRACKING
   Trained model â†’ serving/experiment_tracker.py
   â””â”€ Log to MLflow â†’ model_registry/
   
6. MONITORING
   Production predictions â†’ monitoring/
   â”œâ”€ Drift detection: monitoring/drift_detection/
   â”œâ”€ Anomaly detection: monitoring/anomaly_detector.py
   â””â”€ Alert generation: monitoring/alerts/
   
7. RETRAINING DECISION
   Monitoring alerts â†’ retraining/scheduler.py
   â”œâ”€ Check thresholds from config/
   â”œâ”€ Trigger if conditions met
   â””â”€ Run A/B test: retraining/ab_testing.py
   
8. SERVING
   Production model â† MLflow Registry
   â”œâ”€ Real-time: serving/inference.py
   â”œâ”€ A/B test: serving/ab_test_serving.py
   â””â”€ Predictions â†’ dashboard/utils/data_loader.py
   
9. VISUALIZATION
   Data â†’ dashboards/utils/data_loader.py
   â”œâ”€ Mock fallback if unavailable
   â”œâ”€ Cache 5 minutes
   â””â”€ Display via dashboards/pages/
```

---

## ğŸ“ˆ File Significance & Criticality

### Tier 1 (Critical - System won't run without)
- `orchestration/pipeline.py` - Central orchestrator
- `data_validation/` - Ensures data quality
- `preprocessing/cleaning.py` - Data preparation
- `modeling/models/markov_chain.py` - Core ML model
- `serving/experiment_tracker.py` - Model registry integration

### Tier 2 (Important - Major functionality)
- `monitoring/` - System health
- `retraining/scheduler.py` - Continuous improvement
- `dashboards/app.py` - User interface
- `modeling/evaluation/` - Model validation
- `config/` - System configuration

### Tier 3 (Supporting - Enhanced functionality)
- `eda/` - Data analysis
- `retraining/ab_testing.py` - Deployment validation
- `dashboards/components/` - UI polish
- `utils/` - Helper functions
- `tests/` - Quality assurance

---

## ğŸ”— Module Coupling Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORCHESTRATION (Central)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”œâ†’ data/ (Bronze-Silver-Gold)                          â”‚
â”‚  â”œâ†’ data_validation/ (Quality checks)                   â”‚
â”‚  â”œâ†’ preprocessing/ (Cleaning & enrichment)              â”‚
â”‚  â”œâ†’ modeling/ (Training & evaluation)                   â”‚
â”‚  â”œâ†’ serving/experiment_tracker.py (MLflow logging)      â”‚
â”‚  â”œâ†’ monitoring/ (Drift & anomaly detection)             â”‚
â”‚  â”œâ†’ retraining/scheduler.py (Retraining trigger)        â”‚
â”‚  â””â†’ model_registry/ (Artifact storage)                  â”‚
â”‚                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  SERVING (Inference)                                    â”‚
â”‚  â”œâ†’ model_registry/ (Load model)                        â”‚
â”‚  â”œâ†’ monitoring/ (Log predictions)                       â”‚
â”‚  â””â†’ dashboards/utils/data_loader.py (Feed UI)           â”‚
â”‚                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  MONITORING (Continuous)                                â”‚
â”‚  â”œâ†’ config/ (Thresholds)                                â”‚
â”‚  â”œâ†’ serving/ (Prediction data)                          â”‚
â”‚  â”œâ†’ retraining/scheduler.py (Trigger retraining)        â”‚
â”‚  â”œâ†’ model_registry/ (Log alerts)                        â”‚
â”‚  â””â†’ dashboards/ (Display alerts)                        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary Table

| Folder | Purpose | Key File | Input | Output | Coupling |
|--------|---------|----------|-------|--------|----------|
| `data/` | Data storage | N/A | Raw data | Bronzeâ†’Silverâ†’Gold | All modules |
| `dashboards/` | UI/Visualization | app.py | Model registry | Web interface | Data loader |
| `data_validation/` | Data QA | validate_*.py | Each layer data | Quality score | Preprocessing |
| `eda/` | Data analysis | *.py | Each layer | Statistics | Manual review |
| `config/` | Configuration | *.yaml | N/A | Config objects | All modules |
| `preprocessing/` | Data cleaning | cleaning.py | Bronze data | Silverâ†’Gold | Validation |
| `modeling/` | ML training | markov_chain.py | Gold data | Trained model | Experiment tracker |
| `monitoring/` | Health checks | anomaly_detector.py | Predictions | Alerts | Retraining scheduler |
| `retraining/` | Model updates | scheduler.py | Metrics | New model | A/B testing |
| `serving/` | Inference | experiment_tracker.py | Model registry | Predictions | Monitoring |
| `orchestration/` | DAG | pipeline.py | All components | Workflow | All modules |
| `model_registry/` | Model storage | mlflow.db | Trained models | Model artifacts | Serving |
| `logs/` | Logging | *.log | All modules | Log files | All modules |
| `tests/` | Testing | test_*.py | Components | Test results | CI/CD |
| `utils/` | Helpers | *.py | N/A | Utilities | All modules |

---

This guide provides a comprehensive map of every folder and file, their purposes, interactions, and significance within the FINML ML system.
